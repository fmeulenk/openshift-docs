[[admin-guide-backup-and-restore]]
= Backup and Restore
{product-author}
{product-version}
:data-uri:
:icons: font
:experimental:
:toc: macro
:toc-title:
:prewrap!:

toc::[]

== Overview

In {product-title}, you can _back up_ (saving state to separate storage) and
_restore_ (recreating state from separate storage) at the cluster level. There
is also some preliminary support for xref:project-backup[per-project backup].
The full state of a cluster installation includes:

- etcd data on each master
// fmeulenk: Contradiction to just save the data from one master
- API objects
- registry storage
- volume storage

This topic does not cover how to back up and restore
xref:../install_config/persistent_storage/index.adoc#install-config-persistent-storage-index[persistent
storage], as those topics are left to the underlying storage provider.
However, an example on how to perform a *generic* backup of xref:application-data-backup[application data] is
provided in this document.
// fmeulenk: What about the metadata of a PV?
// Should be in ETCd

[[backup-restore-prerequisites]]
== Prerequisites

. Because the restore procedure involves a xref:cluster-restore[complete
reinstallation], save all the files used in the initial installation. This may
include:
+
- *_~/.config/openshift/installer.cfg.yml_* (from the
xref:../install_config/install/quick_install.adoc#install-config-install-quick-install[Quick Installation]
method)
- Ansible playbooks and inventory files (from the
xref:../install_config/install/advanced_install.adoc#install-config-install-advanced-install[Advanced
Installation] method)
- *_/etc/yum.repos.d/ose.repo_* (from the
xref:../install_config/install/disconnected_install.adoc#install-config-install-disconnected-install[Disconnected
Installation] method)

. Backup the procedures for post-installation steps. Some installations may
involve steps that are not included in the installer. This may include changes
to the services outside of the control of {product-title} or the installation of
extra services like monitoring agents.

. Install packages that provide various utility commands:
+
----
# yum install etcd
----
. If using a containerized installation, pull the etcd image instead:
+
----
# docker pull rhel7/etcd
----

// fmeulenk: Will not work on Atomic-Host

Note the location of the *etcd* data directory (or `$ETCD_DATA_DIR` in the
following sections), which depends on how *etcd* is deployed.

[options="header",cols="1,2"]
|===
| Deployment Type| Data Directory

|all-in-one cluster
|*_/var/lib/openshift/openshift.local.etcd_*

|external etcd (not on master)
|*_/var/lib/etcd_*

|embedded etcd (on master)
|*_/var/lib/origin/etcd_*
|===


[[cluster-backup]]
== Cluster Backup

. Save all the certificates and keys, on each master:
+
----
# cd /etc/origin/master
# tar cf /tmp/certs-and-keys-$(hostname).tar \
    master.proxy-client.crt \
    master.proxy-client.key \
    proxyca.crt \
    proxyca.key \
    master.server.crt \
    master.server.key \
    ca.crt \
    ca.key \
    master.etcd-client.crt \
    master.etcd-client.key \
    master.etcd-ca.crt
----

. If *etcd* is running on more than one host, stop it on each host:
+
----
# sudo systemctl stop etcd
----
+
Although this step is not strictly necessary, doing so ensures that the *etcd*
data is fully synchronized.

. Create an *etcd* backup:
+
----
# etcdctl backup \
    --data-dir $ETCD_DATA_DIR \
    --backup-dir $ETCD_DATA_DIR.bak
----
+
[NOTE]
====
If *etcd* is running on more than one host,
the various instances regularly synchronize their data,
so creating a backup for one of them is sufficient.
====

. Create a template for all cluster API objects:
+
====
----
$ oc export all \
    --exact \//<1>
    --all-namespaces \
    --as-template=mycluster \//<2>
    > mycluster.template.yaml
----
<1> Preserve fields that may be cluster specific,
such as service `portalIP` values or generated names.
<2> The output file has `kind: Template` and `metadata.name: mycluster`.
====
+
The object types included in `oc export all` are:

- BuildConfig
- Build
- DeploymentConfig
- ImageStream
- Pod
- ReplicationController
- Route
- Service

[[cluster-restore]]
== Cluster Restore

. Reinstall {product-title}.
+
This should be done in the
xref:../install_config/install/index.adoc#install-config-install-index[same way]
that {product-title} was previously installed.
// fmeulenk: Don't forget to run thru all your manuel post-installation steps

. Restore the certificates and keys, on each master:
+
----
# cd /etc/origin/master
# tar xvf /tmp/certs-and-keys-$(hostname).tar
----

. Restore from the *etcd* backup:
+
----
# mv $ETCD_DATA_DIR $ETCD_DATA_DIR.orig
# cp -Rp $ETCD_DATA_DIR.bak $ETCD_DATA_DIR
# chcon -R --reference $ETCD_DATA_DIR.orig $ETCD_DATA_DIR
# chown -R etcd:etcd $ETCD_DATA_DIR
----

. Create the API objects for the cluster:
+
----
$ oc create -f mycluster.template.yaml
----

[[project-backup]]
== Project Backup

A future release of {product-title} will feature specific support for
per-project back up and restore.

For now, to back up API objects at the project level, use `oc export` for each
object to be saved. For example, to save the deployment configuration `frontend`
in YAML format:

----
$ oc export dc frontend -o yaml > dc-frontend.yaml
----

To back up all of the project (with the exception of cluster objects like
namespaces and projects):

----
$ oc export all -o yaml > project.yaml
----

// fmeulenk: what about scc and rolebindings?

[[application-data-backup]]
=== Application Data
In many cases, application data can be backed up using the `oc rsync` command,
this is assuming `rsync` is installed within the container image.
The Red Hat `rhel7` base image does contain `rsync`, therefore all images that
are based on this contain it as well.

[WARNING]
====
This is a *generic* backup of application data and does not take into account
application specific backup procedures, for example special export/import
procedures for database systems.
====

Other means of backup may exist depending on the type of the persistent Volume
(cinder, nfs, gluster, ...)

The paths to be backed up are also *application specific*. Often you can figure out which path to back up from looking at the
`mountPath` for volumes in the deploymentconfig.

*Example* of backing up a Jenkins deployment's application data:

. Get the application data mountPath from the deploymentconfig
+
----
$ oc export dc/jenkins|grep mountPath
        - mountPath: /var/lib/jenkins
----

. Get the name of the currently running pod
+
----
$ oc get po --selector=deploymentconfig=jenkins
NAME              READY     STATUS    RESTARTS   AGE
jenkins-1-a3347   1/1       Running   0          18h
----

. Use the `oc rsync` command to copy application data
+
----
$ oc rsync jenkins-1-37nux:/var/lib/jenkins /tmp/
----

[NOTE]
====
This kind of application data backup can only be performed while an application
pod is currently running.
====


[[project-restore]]
== Project Restore

To restore a project it is simply enough to recreate the project and re-create all
all the objects that have been exported during the backup.

----
$ oc new-project myproject
$ oc create -f project.yaml
----

[[application-data-restore]]
=== Application Data
Restoring persistent volume content works similar to the backup using
the `oc rsync` tool. +
The same restrictions apply here.

Following again the same Jenkins *example*:

. Verify the backup
+
----
$ ls -la /tmp/jenkins-backup/
total 8
drwxrwxr-x.  3 user     user   20 Sep  6 11:14 .
drwxrwxrwt. 17 root     root 4096 Sep  6 11:16 ..
drwxrwsrwx. 12 user     user 4096 Sep  6 11:14 jenkins
----

. Use the `oc rsync` tool to copy the data into the running pod.
+
----
$ oc rsync /tmp/jenkins-backup/jenkins jenkins-1-37nux:/var/lib
----

[NOTE]
====
Depending on the application it might be required to restart the application.
====

[start=3]
. Restart the application with new data (optional)
+
----
$ oc delete po jenkins-1-37nux
----

Alternatively one can also scale down the deployment to 0 and up again.
----
$ oc scale --replicas=0 dc/jenkins
$ oc scale --replicas=1 dc/jenkins
----
